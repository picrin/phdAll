Hi Everybody,

This report looks very different from the reports in previous weeks.

Following Simon's and John's suggestion I stopped running analyses as a bunch of python & bash scripts (although some bash scripts still remain) and refactored all the code into jupyter notebooks. Because jupyter supports markdown, the report itself is also a part of the notebook.

This proved to be more challenging than I thought, but I believe it was a worthwhile effort.

The main difficulty is that various bits of analysis are written in python and R (and a few remaining lines of bash) and I needed a solution, which could run the right bit of analysis in the right time. While jupyter supports both R and python, it doesn't support mixing them within one notebook. A workaround I eventually managed to figure out, is to split the analysis into multiple notbooks and execute all "auxiliary" notebooks from a "master" notebook using some internal functionality of jupyter. This may not be ideal, but it's the best I could do.

As to the benefits of this approach:

1) Jupyter is interactive, which means I don't have to wait 10 seconds on each execution of each script. This really has big impact on productivity, as each script is executed hundreds of times, i.e. on every small modification of the codebase.
2) Jupyter supports markdown cells, which means I can document what I'm doing as I go.
3) Jupyter supports inline plots. This is another productivity gain, as writing pngs to disk (and correctly naming them!) and then manually attaching them to latex files was slow & error-prone. 
4) It becomes easier to achieve the solution reached by Simon's former student: each compilation of dissertation leads to a full re-run of all notebooks, which in turn leads to fully reproducible research (how did this student deal with software dependencies though? A docker container could be the way...).


